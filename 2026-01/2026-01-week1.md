デフォルト
中身：
S = 事実
A = 判断
R = 影響

Exaple
## 2026-01-05 OCR 404 misdiagnosis

S: OCR requests returned 404 in native app  
A: I checked network logs and realized the API endpoint didn’t exist  
R: built OCR service, feature recovery 100%  

One sentence:
I misdiagnosed an OCR issue as a frontend bug, but logs showed the API didn’t exist, so I built the service and recovered the feature.

---

## 2026-01-05 Gemini OCR rate limit

S: OCR requests started failing due to Gemini API rate limit in production  
A: I identified the bottleneck and switched to a lightweight batching strategy  
R: request failure rate dropped from frequent timeouts to near zero  

## 2026-01-05 OCR selection

S: we needed reliable OCR for kids, but cloud APIs had latency and rate limits  
A: I compared Google Vision and Tesseract and chose Tesseract for on-device stability  
R: OCR works offline and we eliminated API costs and throttling risks  

## 2026-01-05 Why did you create a separate OCR service instead of embedding it in the app?

S: embedding OCR caused crashes and dependency conflicts in Expo  
A: I extracted OCR into a separate backend service with a clean API  
R: mobile app became stable and we can swap OCR engines without touching the client 


## 2026-01-06 

S: Render.com deployed as Node runtime, Tesseract not found  
A: Suspended service, created new service with Docker runtime selected  
R: Tesseract installed successfully via Dockerfile, OCR working  


## 2026-01-08 OCR selection

S:  
We needed fast, stable OCR for a kids learning app, but cloud OCR APIs (Gemini Vision, Google Vision, Tesseract on server)
introduced high latency, API limits, deployment complexity, and frequent failures (500 / timeout / broken JSON).

A:  
I explored multiple approaches:
- Gemini Vision API via Node server
- Tesseract in Docker on Render
- Multiple environment refactors with Expo + external OCR server

After repeated failures, I pivoted to on-device OCR using Apple Vision via a single Swift native module.

R:  
- OCR now runs locally on iOS with near-zero latency.
- No images are sent to any server (privacy-safe for children).
- Only extracted text is sent to Gemini for formatting.
- Eliminated all OCR servers, Docker, Render, and cloud OCR costs.
- The system became faster, cheaper, more reliable, and simpler to maintain.


## 2026-01-09 OCR selection

S:  
We needed fast, stable OCR for a kids learning app.  
Previous attempts with on-device Apple Vision integration in Expo + Bare Workflow caused prebuild file deletions, null NativeModules, and unstable Simulator behavior.   
Managing Swift modules within Expo proved complex and error-prone.  

A:
- Decided to pivot back to Expo + cloud OCR to simplify development.
- Removed Swift VisionOCR native module and prebuild-sensitive files.
- Planned to use Google Vision API directly from Expo JS, avoiding local Swift integration.
- Ensured fallback for Simulator / camera-unavailable situations by selecting existing photos.

R:  
- OCR system no longer depends on fragile native modules.
- Expo project can now be built and run reliably without prebuild deletions or null NativeModules.
- Development complexity, build errors, and Simulator issues were eliminated.
- Cloud OCR provides stable text extraction, with acceptable latency and simplified maintenance.
- Team can now focus on app features instead of fighting native integration issues.
